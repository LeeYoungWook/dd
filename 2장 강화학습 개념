{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNl44JUkjSsCUd3plo4TYhS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeYoungWook/dd/blob/master/2%EC%9E%A5%20%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%20%EA%B0%9C%EB%85%90\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcoChm6ylArO",
        "colab_type": "text"
      },
      "source": [
        "# **강화학습 개념**\n",
        "\n",
        "최대의 보상을 얻기 위해 시간 순서대로 시스템에 가해지는 행동을 선택하는것이다.\n",
        "보상이 달라지면 정책과 행동또한 달라진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxPtYvgockLU",
        "colab_type": "text"
      },
      "source": [
        "# **마르코프 결정 프로세스**\n",
        "\n",
        "# 1. 정의\n",
        "\n",
        "# (1)구성요소\n",
        "\n",
        "상태($x_t$),  행동( ut ), 보상함수(r( xt,ut ), 상태천이 확률밀도함수(p)\n",
        "\n",
        "# (2) 의미\n",
        "순차적으로 행동을 결정해야 하는 문제를 풀기 위한 수학 모델이다.\n",
        "상태와 행동은 연속공간or이산공간 랜덤변수다. 이산공간 랜덤 변수일 경우 확률밀도함수는 확률로, 적분은 합으로 해석한다,\n",
        "\n",
        "# (3) 파라미터의 관계\n",
        "조건부 확률밀도함수 π(ut∣xt)=p(ut∣xt)를 구하는 것이 목적이다.\n",
        "\n",
        "π(ut∣xt)를 정책이라고 한다.\n",
        "\n",
        "상태에서 에이전트가 행동을 선택했을때 다음 상태의 확률밀도 함수는 p(xt+1∣xt,ut)다\n",
        "\n",
        " 1. 상태변수x0에서 어떤 정책π에 의하여 행동 ut로 상태변수 x1로 이동하고 보상 r(x0,u0)을 얻는다\n",
        " 2. 상태변수x1에서 어떤 정책에 의하여 행동ut로 상태변수 x3로 이동하고 보상 r(x1,u1)을 얻는다\n",
        " 3. 상태변수 행동 보상의 순서로 반복되어 전개된다.\n",
        "즉. 궤적함수는 \n",
        "τ=(x0,u0,x1,u1,…,xT,uT) 로 연속적인 시퀀스로 구성된다 \n",
        "\n",
        "\n",
        "# 2. 확률적 MDP\n",
        "\n",
        "위의 방식처럼 환경 모델이 상태천이 확률밀도함수로 환경 모델이 주어지는 경우이다.\n",
        "\n",
        "\n",
        "# 3. 확정적 MDP\n",
        "\n",
        "환경 모델과 정책이 모두 확정적으로 주어진것이다.\n",
        "이 경우 보상함수는 자연스럽게 확정적인 함수가 된다.\n",
        "\n",
        "# (1) 에피소드\n",
        "정책π를 실행하여 상태변수,행동,보상이 x0->u0->r(x0,u0)->x1->u1->r(x1,u1)->순서로 전개 된다 \n",
        "\n",
        "시간스탭 t에서 상태와 행동이 주어지면 다음 상태 xt+1=f(xt,ut) 를 확정적으로 알 수있다. 이 경우 보상은 r(xt,ut)로 주어지고 정책은 상태변수에서 행동을 직접 계산하는 함수 )이ut=π(xt다.\n",
        "\n",
        "# 4. 확정적 MDP 와 확률적 MDP의 차이\n",
        "\n",
        "행동을 취하기 위한 정책을 구하는 것에서 차이가 난다.\n",
        "\n",
        "확률적 MDP는  확률밀도 함수 p(xt+1∣xt,ut)로 정책을 구한다.\n",
        "확정점 MDP는 상태변수에서 ut=π(xㅅ)로 직접 행동을 계산할 수 있다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsG7SJV-kJnr",
        "colab_type": "text"
      },
      "source": [
        "# **가치함수**\n",
        "\n",
        "# 1.상태가치\n",
        "어떤 상태변수 xt에서 시작해 그로부터 정책π에 의해서 행동이 가해졌을 떄  기대할 수 있는 반환값을 상태가치라고 한다.\n",
        "\n",
        "# 2.행동가치\n",
        "어떤 상태변수xt에서 행동ut를 선택하고 그로부터 정책 π에 의해서 행동이 가해졌을때 기대 할 수 있는 미래의 반한값을 행동가치라고 한다.\n",
        "\n",
        "# 3. 상태가치와 행동가치의 관계\n",
        "상태가치는 상태변수 xt에서 선택 가능한 모든 행동 ut에 대한 행동가치의 평균값임을 알 수 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24TbPGThm8DR",
        "colab_type": "text"
      },
      "source": [
        "# **벨만 방적식**\n",
        "\n",
        "모든 정책 중에서 상태가치의 값을 최대로 만드는 정책을 적용했을 때를 최적 상태가치 함수와 최적 행동가치 함수라고 한다.\n",
        "또한 상태가치 값을 최대로 만드는 정책을 최적 정책이라고 한다.\n",
        "최적정책은 최적 행동가치 함수로 표현 할 수있다.\n",
        "\n",
        "# **# 결론**\n",
        "최대의 보상을 얻는 행동을 취하기 위해 최적 정책을 찾아야한다.\n",
        "이때, 모델기반 강화학습 or 직접 정책의 유도 or 가치 함수 추정을 이용할 수 있다."
      ]
    }
  ]
}